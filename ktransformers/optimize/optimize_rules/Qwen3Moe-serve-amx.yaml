# Qwen3 MoE AMX优化配置文件
# 此配置针对Qwen3 MoE模型使用Intel AMX指令集进行优化
# 特别适用于支持AMX指令集的Intel处理器(如Sapphire Rapids)
# 通过AMX加速，可在保持精度的同时显著提高CPU侧的专家计算性能

# === 位置编码配置 ===
# 将Qwen2MoeRotaryEmbedding替换为更通用高效的RotaryEmbedding实现
- match:
    class: ktransformers.models.modeling_qwen2_moe.Qwen2MoeRotaryEmbedding
  replace:
    class: ktransformers.operators.RoPE.RotaryEmbedding
    kwargs:
      generate_device: "cuda"  # 生成阶段在GPU上执行
      prefill_device: "cuda"   # 预填充阶段也在GPU上执行，确保设备一致性

# === 词表映射层配置 ===
# 词表映射层(lm_head)是模型的输出层，优化此层对推理速度有显著影响
- match:
    name: "^lm_head$"  # 精确匹配lm_head层
    class: torch.nn.Linear  # 只匹配PyTorch原生线性层类型
  replace:
    class: ktransformers.operators.linear.KTransformersLinear  # 替换为优化的线性层实现
    kwargs:
      generate_device: "cuda"      # 生成阶段在GPU上执行
      prefill_device: "cuda"       # 预填充阶段也在GPU上执行
      generate_op: "VLinearMarlin" # 使用Marlin变种后端，针对输出层优化
      prefill_op: "KLinearTorch"   # 预填充使用PyTorch后端，提高稳定性

# === 注释掉的全局线性层配置 ===
# 这是另一种可能的线性层优化策略，当前版本不使用这种全局匹配
# 保留这段注释是为了便于未来调整配置策略
# - match:
#     name: "^model\\.layers\\..*$"  # regular expression 
#     class: torch.nn.Linear  # only match modules matching name and class simultaneously
#   replace:
#     class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types
#     kwargs:
#       generate_device: "cuda"
#       prefill_device: "cuda"
#       generate_op: "VLinearMarlin"
#       prefill_op: "KLinearTorch"

# === 精细线性层配置 ===
# 为模型中所有线性层提供优化，但排除了专家门控层(mlp.shared_expert_gate)
- match:
    name: "^model\\.layers\\.(?!.*mlp\\.shared_expert_gate).*$"  # 匹配所有层中的线性层，但排除专家门控
    class: torch.nn.Linear  # 只匹配PyTorch线性层
  replace:
    class: ktransformers.operators.linear.KTransformersLinear  # 替换为优化的线性层实现
    kwargs:
      generate_device: "cuda"      # 生成阶段在GPU上执行
      prefill_device: "cuda"       # 预填充阶段也在GPU上执行
      generate_op: "KLinearMarlin" # 使用Marlin后端，基于CUTLASS优化
      prefill_op: "KLinearTorch"   # 预填充使用PyTorch后端，提高稳定性

# === MoE块整体配置 ===
# 将整个MoE模块替换为优化版本，更好地管理专家的选择和计算
- match:
    name: "^model\\.layers\\..*\\.mlp$"
    class: ktransformers.models.modeling_qwen3_moe.Qwen3MoeSparseMoeBlock
  replace:
    class: ktransformers.operators.experts.KQwen3MoeSparseMoeBlockV2  # 优化的MoE实现
    kwargs:
      generate_device: "cuda"  # 生成阶段在GPU上执行
      prefill_device: "cuda"   # 预填充阶段也在GPU上执行

# === 专家模块AMX优化配置 ===
# 专家模块是MoE架构中计算量最大的部分，使用AMX指令集显著加速CPU计算
- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExpertsV2  # 支持专家并行的优化实现
    kwargs:
      prefill_device: "cuda"       # 预填充阶段在GPU上执行
      prefill_op: "KExpertsTorch"  # 预填充使用PyTorch后端
      generate_device: "cpu"       # 生成阶段放在CPU上执行，节省GPU内存
      generate_op: "KExpertsCPU"   # 使用CPU优化的后端
      out_device: "cuda"           # 计算结果发送回GPU继续处理
      backend: "AMXBF16"           # 使用AMX BF16后端，支持bfloat16精度的AMX加速
                                    # 可选项: "AMXBF16", "AMXInt8", "llamafile"(默认)
  recursive: False                  # 不递归处理子模块，避免过度注入

# === 注意力层配置 ===
# 在服务场景下，使用专门针对Qwen3 MoE优化的注意力实现
- match:
    name: "^model\\.layers\\..*\\.self_attn$"
  replace:
    class: ktransformers.operators.balance_serve_attention.KQwen3MoeAttention  # 优化的注意力机制实现
    kwargs:
      generate_device: "cuda"  # 生成阶段在GPU上执行
      prefill_device: "cuda"   # 预填充阶段也在GPU上执行

# === 模型整体配置 ===
# 为整个模型对象注入优化，提供全局控制和优化策略
- match:
    name: "^model$"
  replace:
    class: "ktransformers.operators.models.KQwen2MoeModel"  # 优化的Qwen2 MoE模型实现
    kwargs:
      per_layer_prefill_intput_threshold: 0  # 设置为0关闭逐层预填充，降低内存波动

# === 词嵌入层配置 ===
# 将词嵌入层放在CPU上可以节省GPU内存，因为它只在输入时使用一次
- match:
    name: "^model.embed_tokens"
  replace:
    class: "default"      # 保持原始类不变，只修改运行设备
    kwargs:
      generate_device: "cpu"  # 生成阶段在CPU上运行
      prefill_device: "cpu"   # 预填充阶段也在CPU上运行

# === 规范化层配置 ===
# RMSNorm是Qwen3模型使用的规范化方法，比传统LayerNorm计算效率更高
- match:
    class: ktransformers.models.modeling_qwen3_moe.Qwen3MoeRMSNorm
  replace:
    class: ktransformers.operators.layernorm.KQwen3MoeRMSNorm  # 优化的RMSNorm实现
    kwargs:
      generate_device: "cuda"  # 生成阶段在GPU上执行
      prefill_device: "cuda"   # 预填充阶段也在GPU上执行

# === MLP模块配置 ===
# 优化非专家计算的MLP模块，提高整体性能
- match:
    class: ktransformers.models.modeling_qwen3_moe.Qwen3MoeMLP
  replace:
    class:  ktransformers.operators.mlp.KQwen2MoeMLP  # 优化的MLP实现
    kwargs:
      generate_device: "cuda"  # 生成阶段在GPU上执行
      prefill_device: "cuda"   # 预填充阶段也在GPU上执行