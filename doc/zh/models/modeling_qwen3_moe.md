# Qwen3Moeæ¨¡åž‹å®žçŽ°

## æ¦‚è¿°
æœ¬æ–‡æ¡£æä¾›äº†`modeling_qwen3_moe.py`æ–‡ä»¶ä¸­é‡è¦æ³¨é‡Šçš„ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬ï¼Œå¸®åŠ©ä¸­æ–‡ç”¨æˆ·æ›´å¥½åœ°ç†è§£Qwen3 MoEæ¨¡åž‹çš„å®žçŽ°ç»†èŠ‚ã€‚

## åŽŸå§‹æ–‡ä»¶æ³¨é‡Šä¸­æ–‡ç¿»è¯‘

```python
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           æ­¤æ–‡ä»¶æ˜¯ä»Žsrc/transformers/models/qwen3_moe/modular_qwen3_moe.pyè‡ªåŠ¨ç”Ÿæˆçš„ã€‚
#               è¯·å‹¿æ‰‹åŠ¨ç¼–è¾‘æ­¤æ–‡ä»¶ï¼Œå› ä¸ºä»»ä½•ç¼–è¾‘éƒ½å°†åœ¨ç”Ÿæˆæ–‡ä»¶æ—¶è¢«è¦†ç›–ã€‚
#             å¦‚æžœéœ€è¦è¿›è¡Œä»»ä½•æ›´æ”¹ï¼Œè¯·ç›´æŽ¥åœ¨modular_qwen3_moe.pyæ–‡ä»¶ä¸­åº”ç”¨æ›´æ”¹ã€‚æˆ‘ä»¬çš„CIä¼šå¼ºåˆ¶æ‰§è¡Œè¿™ä¸€ç‚¹ã€‚
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
```

### é‡è¦å‡½æ•°æ³¨é‡Š

```python
def rotate_half(x):
    """æ—‹è½¬è¾“å…¥çš„ä¸€åŠéšè—ç»´åº¦ã€‚"""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """å°†æ—‹è½¬ä½ç½®åµŒå…¥åº”ç”¨äºŽæŸ¥è¯¢å’Œé”®å¼ é‡ã€‚

    å‚æ•°:
        q (`torch.Tensor`): æŸ¥è¯¢å¼ é‡ã€‚
        k (`torch.Tensor`): é”®å¼ é‡ã€‚
        cos (`torch.Tensor`): æ—‹è½¬åµŒå…¥çš„ä½™å¼¦éƒ¨åˆ†ã€‚
        sin (`torch.Tensor`): æ—‹è½¬åµŒå…¥çš„æ­£å¼¦éƒ¨åˆ†ã€‚
        position_ids (`torch.Tensor`, *å¯é€‰*):
            å·²å¼ƒç”¨ä¸”æœªä½¿ç”¨ã€‚
        unsqueeze_dim (`int`, *å¯é€‰*, é»˜è®¤ä¸º1):
            'unsqueeze_dim'å‚æ•°æŒ‡å®šäº†æ²¿å“ªä¸ªç»´åº¦å¯¹cos[position_ids]å’Œsin[position_ids]è¿›è¡Œæ‰©å±•ï¼Œ
            ä½¿å®ƒä»¬èƒ½å¤Ÿæ­£ç¡®åœ°å¹¿æ’­åˆ°qå’Œkçš„ç»´åº¦ã€‚ä¾‹å¦‚ï¼Œæ³¨æ„cos[position_ids]å’Œsin[position_ids]çš„å½¢çŠ¶ä¸º
            [batch_size, seq_len, head_dim]ã€‚ç„¶åŽï¼Œå¦‚æžœqå’Œkçš„å½¢çŠ¶ä¸º[batch_size, heads, seq_len, head_dim]ï¼Œ
            è®¾ç½®unsqueeze_dim=1ä½¿cos[position_ids]å’Œsin[position_ids]å¯ä»¥å¹¿æ’­åˆ°qå’Œkçš„å½¢çŠ¶ã€‚
            ç±»ä¼¼åœ°ï¼Œå¦‚æžœqå’Œkçš„å½¢çŠ¶ä¸º[batch_size, seq_len, heads, head_dim]ï¼Œåˆ™è®¾ç½®unsqueeze_dim=2ã€‚
    è¿”å›ž:
        åŒ…å«ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥æ—‹è½¬çš„æŸ¥è¯¢å’Œé”®å¼ é‡çš„`tuple(torch.Tensor)`ã€‚
    """


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    è¿™ç­‰åŒäºŽtorch.repeat_interleave(x, dim=1, repeats=n_rep)ã€‚éšè—çŠ¶æ€ä»Ž(batch, num_key_value_heads, seqlen, head_dim)
    å˜ä¸º(batch, num_attention_heads, seqlen, head_dim)
    """


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    """æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„å‰å‘ä¼ æ’­å®žçŽ°"""


def load_balancing_loss_func(
    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],
    num_experts: Optional[int] = None,
    top_k=2,
    attention_mask: Optional[torch.Tensor] = None,
) -> Union[torch.Tensor, int]:
    r"""
    è®¡ç®—Switch Transformerä¸­çš„è¾…åŠ©è´Ÿè½½å¹³è¡¡æŸå¤± - ç”¨PyTorchå®žçŽ°ã€‚

    å‚è§Switch Transformer(https://arxiv.org/abs/2101.03961)äº†è§£æ›´å¤šè¯¦æƒ…ã€‚è¿™ä¸ªå‡½æ•°å®žçŽ°äº†è®ºæ–‡ä¸­æ–¹ç¨‹(4)-(6)
    ä¸­å‘ˆçŽ°çš„æŸå¤±å‡½æ•°ã€‚å®ƒæ—¨åœ¨æƒ©ç½šä¸“å®¶ä¹‹é—´è·¯ç”±ä¸å¹³è¡¡çš„æƒ…å†µã€‚

    å‚æ•°:
        gate_logits:
            æ¥è‡ª`gate`çš„logitsï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªmodel.config.num_hidden_layerså¼ é‡çš„å…ƒç»„ï¼Œ
            å½¢çŠ¶ä¸º[batch_size X sequence_length, num_experts]ã€‚
        num_experts:
            ä¸“å®¶æ•°é‡
        top_k:
            æ¯ä¸ªtokenè·¯ç”±çš„ä¸“å®¶æ•°é‡ï¼Œä¹Ÿå¯ä»¥è§£é‡Šä¸º`top-k`è·¯ç”±å‚æ•°ã€‚
        attention_mask (`torch.Tensor`, *å¯é€‰*):
            åœ¨å‰å‘å‡½æ•°ä¸­ä½¿ç”¨çš„attention_maskï¼Œ
            å½¢çŠ¶ä¸º[batch_size X sequence_length]ï¼ˆå¦‚æžœä¸ä¸ºNoneï¼‰ã€‚

    è¿”å›ž:
        è¾…åŠ©æŸå¤±ã€‚
    """
```

### ç±»æ³¨é‡Š

```python
class Qwen3MoeAttention(nn.Module):
    """æ¥è‡ª'Attention Is All You Need'è®ºæ–‡çš„å¤šå¤´æ³¨æ„åŠ›"""


class Qwen3MoeMLP(nn.Module):
    """æ ‡å‡†çš„MLPå®žçŽ°"""


class Qwen3MoeSparseMoeBlock(nn.Module):
    """ç¨€ç–æ··åˆä¸“å®¶å—å®žçŽ°"""


class Qwen3MoeRMSNorm(nn.Module):
    """
    Qwen3MoeRMSNormç­‰åŒäºŽT5LayerNorm
    """


class Qwen3MoeDecoderLayer(nn.Module):
    """å•ä¸ªQwen3Moeè§£ç å™¨å±‚å®žçŽ°"""


class Qwen3MoeRotaryEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®åµŒå…¥å®žçŽ°"""


class Qwen3MoePreTrainedModel(PreTrainedModel):
    """
    è¿™ä¸ªæ¨¡åž‹ç»§æ‰¿è‡ª[`PreTrainedModel`]ã€‚æŸ¥çœ‹çˆ¶ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡åž‹å®žçŽ°çš„é€šç”¨æ–¹æ³•
    (ä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ã€ä¿®å‰ªå¤´éƒ¨ç­‰)

    è¿™ä¸ªæ¨¡åž‹ä¹Ÿæ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚
    å°†å…¶ä½œä¸ºå¸¸è§„PyTorchæ¨¡å—ä½¿ç”¨ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£äº†è§£æ‰€æœ‰ä¸Žä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚

    å‚æ•°:
        config ([`Qwen3MoeConfig`]):
            æ¨¡åž‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡åž‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸Žæ¨¡åž‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚
            æŸ¥çœ‹[`~PreTrainedModel.from_pretrained`]æ–¹æ³•åŠ è½½æ¨¡åž‹æƒé‡ã€‚
    """


class Qwen3MoeModel(Qwen3MoePreTrainedModel):
    """
    ç”±*config.num_hidden_layers*å±‚ç»„æˆçš„Transformerè§£ç å™¨ã€‚æ¯ä¸€å±‚éƒ½æ˜¯[`Qwen3MoeDecoderLayer`]

    å‚æ•°:
        config: Qwen3MoeConfig
    """


class Qwen3MoeForCausalLM(Qwen3MoePreTrainedModel, GenerationMixin):
    """
    ç”¨äºŽå› æžœè¯­è¨€å»ºæ¨¡çš„Qwen3Moeæ¨¡åž‹
    """


class Qwen3MoeForSequenceClassification(Qwen3MoePreTrainedModel):
    """
    å¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„Qwen3Moeæ¨¡åž‹transformer(é¡¶éƒ¨çš„çº¿æ€§å±‚)ã€‚

    [`Qwen3MoeForSequenceClassification`]ä½¿ç”¨æœ€åŽä¸€ä¸ªtokenè¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æžœæ¨¡åž‹(ä¾‹å¦‚GPT-2)ä¸€æ ·ã€‚

    ç”±äºŽå®ƒå¯¹æœ€åŽä¸€ä¸ªtokenè¿›è¡Œåˆ†ç±»ï¼Œå®ƒéœ€è¦çŸ¥é“æœ€åŽä¸€ä¸ªtokençš„ä½ç½®ã€‚å¦‚æžœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œ
    å®ƒä¼šåœ¨æ¯ä¸€è¡Œä¸­æ‰¾åˆ°ä¸æ˜¯å¡«å……tokençš„æœ€åŽä¸€ä¸ªtokenã€‚å¦‚æžœæ²¡æœ‰å®šä¹‰`pad_token_id`ï¼Œå®ƒç®€å•åœ°å–æ¯ä¸€è¡Œ
    æ‰¹æ¬¡ä¸­çš„æœ€åŽä¸€ä¸ªå€¼ã€‚ç”±äºŽå½“ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶å®ƒæ— æ³•çŒœæµ‹å¡«å……tokensï¼Œ
    å®ƒä¼šåšåŒæ ·çš„äº‹æƒ…(å–æ‰¹æ¬¡ä¸­æ¯ä¸€è¡Œçš„æœ€åŽä¸€ä¸ªå€¼)ã€‚
    """


class Qwen3MoeForTokenClassification(Qwen3MoePreTrainedModel):
    """
    å¸¦æœ‰tokenåˆ†ç±»å¤´çš„Qwen3Moeæ¨¡åž‹transformer(é¡¶éƒ¨éšè—çŠ¶æ€è¾“å‡ºä¸Šçš„çº¿æ€§å±‚)ï¼Œä¾‹å¦‚ç”¨äºŽå‘½åå®žä½“è¯†åˆ«(NER)ä»»åŠ¡ã€‚
    """


class Qwen3MoeForQuestionAnswering(Qwen3MoePreTrainedModel):
    """
    å¸¦æœ‰è·¨åº¦åˆ†ç±»å¤´çš„Qwen3Moeæ¨¡åž‹transformerï¼Œç”¨äºŽæå–å¼é—®ç­”ä»»åŠ¡ï¼Œå¦‚SQuAD(é¡¶éƒ¨éšè—çŠ¶æ€è¾“å‡ºä¸Šçš„çº¿æ€§å±‚ï¼Œ
    ç”¨äºŽè®¡ç®—`è·¨åº¦å¼€å§‹logits`å’Œ`è·¨åº¦ç»“æŸlogits`)ã€‚
    """
```

### è¾“å…¥æ–‡æ¡£å­—ç¬¦ä¸² (DOCSTRING)

```python
QWEN3_MOE_INPUTS_DOCSTRING = r"""
    å‚æ•°:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—tokensçš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½ æä¾›çš„å¡«å……å°†è¢«å¿½ç•¥ã€‚

            ç´¢å¼•å¯ä»¥ä½¿ç”¨[`AutoTokenizer`]èŽ·å–ã€‚å‚è§[`PreTrainedTokenizer.encode`]å’Œ
            [`PreTrainedTokenizer.__call__`]èŽ·å–è¯¦ç»†ä¿¡æ¯ã€‚

            [ä»€ä¹ˆæ˜¯input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*):
            é¿å…åœ¨å¡«å……tokenç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æŽ©ç ã€‚æŽ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©:

            - 1è¡¨ç¤º**æœªè¢«é®è”½**çš„tokens,
            - 0è¡¨ç¤º**è¢«é®è”½**çš„tokensã€‚

            [ä»€ä¹ˆæ˜¯attention masks?](../glossary#attention-mask)

            ç´¢å¼•å¯ä»¥ä½¿ç”¨[`AutoTokenizer`]èŽ·å–ã€‚å‚è§[`PreTrainedTokenizer.encode`]å’Œ
            [`PreTrainedTokenizer.__call__`]èŽ·å–è¯¦ç»†ä¿¡æ¯ã€‚

            å¦‚æžœä½¿ç”¨äº†`past_key_values`ï¼Œå¯é€‰æ‹©æ€§åœ°åªè¾“å…¥æœ€åŽçš„`input_ids`(å‚è§`past_key_values`)ã€‚

            å¦‚æžœä½ æƒ³æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œä½ åº”è¯¥é˜…è¯»[`modeling_opt._prepare_decoder_attention_mask`]
            å¹¶æŒ‰ä½ çš„éœ€è¦ä¿®æ”¹ã€‚å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾1ï¼Œ
            äº†è§£æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ã€‚

            - 1è¡¨ç¤ºå¤´éƒ¨**æœªè¢«é®è”½**,
            - 0è¡¨ç¤ºå¤´éƒ¨**è¢«é®è”½**ã€‚
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*):
            æ¯ä¸ªè¾“å…¥åºåˆ—tokenåœ¨ä½ç½®åµŒå…¥ä¸­çš„ç´¢å¼•ã€‚åœ¨`[0, config.n_positions - 1]`èŒƒå›´å†…é€‰æ‹©ã€‚

            [ä»€ä¹ˆæ˜¯position IDs?](../glossary#position-ids)
        past_key_values (`Cache`, *å¯é€‰*):
            é¢„å…ˆè®¡ç®—å¥½çš„éšè—çŠ¶æ€(è‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„keyå’Œvalue)ï¼Œå¯ç”¨äºŽåŠ é€Ÿé¡ºåºè§£ç ã€‚
            è¿™é€šå¸¸åŒ…æ‹¬åœ¨`use_cache=True`æˆ–`config.use_cache=True`æ—¶ç”±æ¨¡åž‹åœ¨ä¸Šä¸€é˜¶æ®µè§£ç è¿”å›žçš„`past_key_values`ã€‚

            å®ƒæ˜¯ä¸€ä¸ª[`~cache_utils.Cache`]å®žä¾‹ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è§æˆ‘ä»¬çš„[kv cacheæŒ‡å—](https://huggingface.co/docs/transformers/en/kv_cache)ã€‚

            å¦‚æžœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©æ€§åœ°åªè¾“å…¥æœ€åŽçš„`input_ids`(é‚£äº›æ²¡æœ‰ç»™è¯¥æ¨¡åž‹çš„past key value statesçš„è¾“å…¥)ï¼Œ
            å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯æ‰€æœ‰`input_ids`çš„å½¢çŠ¶`(batch_size, sequence_length)`ã€‚
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *å¯é€‰*):
            å¯é€‰æ‹©ç›´æŽ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æžœä½ æƒ³å¯¹å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡
            æœ‰æ›´å¤šæŽ§åˆ¶ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡åž‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚
        use_cache (`bool`, *å¯é€‰*):
            å¦‚æžœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›ž`past_key_values` key valueçŠ¶æ€ï¼Œå¯ç”¨äºŽåŠ é€Ÿè§£ç (å‚è§`past_key_values`)ã€‚
        output_attentions (`bool`, *å¯é€‰*):
            æ˜¯å¦è¿”å›žæ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›žçš„å¼ é‡ä¸‹çš„`attentions`ã€‚
        output_hidden_states (`bool`, *å¯é€‰*):
            æ˜¯å¦è¿”å›žæ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›žçš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚
        return_dict (`bool`, *å¯é€‰*):
            æ˜¯å¦è¿”å›ž[`~utils.ModelOutput`]è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *å¯é€‰*):
            æè¿°è¾“å…¥åºåˆ—tokensåœ¨åºåˆ—ä¸­çš„ä½ç½®çš„ç´¢å¼•ã€‚ä¸Ž`position_ids`ç›¸åï¼Œæ­¤å¼ é‡ä¸å—å¡«å……å½±å“ã€‚
            å®ƒç”¨äºŽåœ¨æ­£ç¡®ä½ç½®æ›´æ–°ç¼“å­˜å¹¶æŽ¨æ–­å®Œæ•´çš„åºåˆ—é•¿åº¦ã€‚
"""
```

### ç”Ÿæˆæ–¹æ³•æ–‡æ¡£è¯´æ˜Ž

```python
def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    output_router_logits: Optional[bool] = None,
    return_dict: Optional[bool] = None,
    cache_position: Optional[torch.LongTensor] = None,
    logits_to_keep: Union[int, torch.Tensor] = 0,
    # **kwargs: Unpack[KwargsForCausalLM],
) -> Union[Tuple, CausalLMOutputWithPast]:
    r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*):
            ç”¨äºŽè®¡ç®—æŽ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…ï¼Œ
            æˆ–è€…ä¸º-100(å‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²)ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„tokenså°†è¢«å¿½ç•¥(æŽ©ç )ï¼Œ
            åªæœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…çš„tokensæ‰ä¼šè®¡ç®—æŸå¤±ã€‚

        logits_to_keep (`int`æˆ–`torch.Tensor`, *å¯é€‰*):
            å¦‚æžœæ˜¯`int`ï¼Œä¸ºæœ€åŽ`logits_to_keep`ä¸ªtokensè®¡ç®—logitsã€‚å¦‚æžœæ˜¯`0`ï¼Œä¸ºæ‰€æœ‰`input_ids`è®¡ç®—logits(ç‰¹æ®Šæƒ…å†µ)ã€‚
            å¯¹äºŽç”Ÿæˆï¼Œåªéœ€è¦æœ€åŽä¸€ä¸ªtokençš„logitsï¼Œåªä¸ºè¯¥tokenè®¡ç®—logitså¯ä»¥èŠ‚çœå†…å­˜ï¼Œ
            è¿™å¯¹äºŽé•¿åºåˆ—æˆ–å¤§è¯æ±‡é‡æ¥è¯´å˜å¾—éžå¸¸æ˜¾è‘—ã€‚
            å¦‚æžœæ˜¯`torch.Tensor`ï¼Œå¿…é¡»æ˜¯ä¸€ç»´çš„ï¼Œå¯¹åº”äºŽåºåˆ—é•¿åº¦ç»´åº¦ä¸­è¦ä¿ç•™çš„ç´¢å¼•ã€‚
            å½“ä½¿ç”¨æ‰“åŒ…å¼ é‡æ ¼å¼(æ‰¹æ¬¡å’Œåºåˆ—é•¿åº¦çš„å•ä¸€ç»´åº¦)æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚

    è¿”å›ž:

    ç¤ºä¾‹:

    ```python
    >>> from transformers import AutoTokenizer, Qwen3MoeForCausalLM

    >>> model = Qwen3MoeForCausalLM.from_pretrained("Qwen/Qwen3-MoE-15B-A2B")
    >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-MoE-15B-A2B")

    >>> prompt = "Hey, are you conscious? Can you talk to me?"
    >>> inputs = tokenizer(prompt, return_tensors="pt")

    >>> # ç”Ÿæˆ
    >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
    >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
    ```"""
``` 